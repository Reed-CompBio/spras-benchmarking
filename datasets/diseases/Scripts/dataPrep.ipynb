{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021d0f92",
   "metadata": {},
   "source": [
    "# Read and crop input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f14ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "\n",
    "text_mining = pd.read_csv('../raw/human_disease_textmining_filtered.tsv',sep = '\\t')\n",
    "text_mining.columns= ['geneID','geneName','diseaseID','diseaseName','zScore','confidenceScore','sourceUrl']\n",
    "\n",
    "knowlege = pd.read_csv('../raw/human_disease_knowledge_filtered.tsv',sep = '\\t')\n",
    "knowlege.columns= ['geneID','geneName','diseaseID','diseaseName','sourceDB','evidenceType','confidenceScore']\n",
    "\n",
    "tiga = pd.read_csv('../raw/tiga_gene-trait_stats.tsv',sep='\\t')\n",
    "tiga = tiga[['ensemblId','efoId','trait','n_snp','n_snpw']]\n",
    "tiga = tiga.drop_duplicates(subset=['ensemblId','trait'])\n",
    "\n",
    "human_do = pd.read_csv('../raw/HumanDO.tsv',sep = '\\t')\n",
    "human_do = human_do.drop_duplicates(subset='label')\n",
    "human_do = human_do[['id','label']]\n",
    "\n",
    "tiga_do = tiga.merge(human_do,left_on='trait',right_on='label',how='inner',validate='m:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e4f93",
   "metadata": {},
   "source": [
    "# Map geneIDs in knowledge channel from ENSP to ENSG format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecbcb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "url='https://biit.cs.ut.ee/gprofiler/api/convert/convert/',\n",
    "json={\n",
    "    'organism':'hsapiens',\n",
    "    'target':'ENSG',\n",
    "    'query': list(knowlege['geneID']),\n",
    "    }\n",
    ")\n",
    "\n",
    "results = r.json()['result']\n",
    "mapping ={}\n",
    "for x in results:\n",
    "    if x['converted'] != 'None':\n",
    "        mapping.update({x['incoming']:x['converted']})\n",
    "\n",
    "mapping_df = pd.DataFrame.from_dict(mapping.items())\n",
    "mapping_df.columns = ['ENSP','ENSG']\n",
    "knowlege_mapped = knowlege.merge(mapping_df,left_on='geneID',right_on='ENSP',how ='inner',validate='m:1')\n",
    "knowlege_mapped = knowlege_mapped.sort_values('confidenceScore',ascending=False).drop_duplicates(subset = ['ENSG','diseaseID'],keep=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4545f1",
   "metadata": {},
   "source": [
    "# Map geneIDs in text mining channel from ENSP to ENSG format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67440f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "url='https://biit.cs.ut.ee/gprofiler/api/convert/convert/',\n",
    "json={\n",
    "    'organism':'hsapiens',\n",
    "    'target':'ENSG',\n",
    "    'query': list(text_mining['geneID']),\n",
    "    }\n",
    ")\n",
    "results = r.json()['result']\n",
    "mapping ={}\n",
    "for x in results:\n",
    "    if x['converted'] != 'None':\n",
    "        mapping.update({x['incoming']:x['converted']})\n",
    "\n",
    "mapping_df = pd.DataFrame.from_dict(mapping.items())\n",
    "mapping_df.columns = ['ENSP','ENSG']\n",
    "text_mining_mapped = text_mining.merge(mapping_df,left_on='geneID',right_on='ENSP',how ='inner',validate='m:1')\n",
    "text_mining_mapped = text_mining_mapped.sort_values('confidenceScore',ascending=False).drop_duplicates(subset = ['ENSG','diseaseID'],keep=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adcc146",
   "metadata": {},
   "source": [
    "# Select highest confidence scores for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25180af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inner = text_mining_mapped.merge(knowlege_mapped,on=['ENSG','diseaseID'],how='inner')\n",
    "inner['confidenceScore'] = inner.apply(lambda x: max(x.confidenceScore_x,x.confidenceScore_y),axis=1)\n",
    "inner = inner.rename(columns={'ENSP_x':'ENSP','geneName_x':'geneName','diseaseName_x':'diseaseName','geneID_x':'geneID'})\n",
    "inner = inner[['ENSG','ENSP','geneName','diseaseID','diseaseName','confidenceScore']]\n",
    "\n",
    "\n",
    "txt_only = text_mining_mapped.merge(knowlege_mapped,on=['ENSG','diseaseID'],how='left')\n",
    "txt_only = txt_only[txt_only['confidenceScore_y'].isna()]\n",
    "txt_only = txt_only.rename(columns={'confidenceScore_x':'confidenceScore','ENSP_x':'ENSP','geneName_x':'geneName','diseaseName_x':'diseaseName'})\n",
    "txt_only = txt_only[['ENSG','ENSP','geneName','diseaseID','diseaseName','confidenceScore']]\n",
    "\n",
    "\n",
    "kn_only = text_mining_mapped.merge(knowlege_mapped,on=['ENSG','diseaseID'],how='right')\n",
    "kn_only = kn_only[kn_only['confidenceScore_x'].isna()]\n",
    "kn_only = kn_only.rename(columns={'confidenceScore_y':'confidenceScore','ENSP_y':'ENSP','geneName_y':'geneName','diseaseName_y':'diseaseName'})\n",
    "kn_only = kn_only[['ENSG','ENSP','geneName','diseaseID','diseaseName','confidenceScore']]\n",
    "\n",
    "\n",
    "df_list = [inner,txt_only,kn_only]\n",
    "inputs = pd.concat(df_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc64004",
   "metadata": {},
   "source": [
    "# Generate quantile stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f683eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count quantiles:  [2.0, 5.0, 20.0]\n",
      "score quantiles:  [1.643, 1.841, 2.165]\n"
     ]
    }
   ],
   "source": [
    "import statistics as stats\n",
    "\n",
    "inputs_group = inputs.groupby('diseaseName')\n",
    "inputs_dict = {k:v for k,v in inputs_group}\n",
    "inputs_count = {x:len(inputs_dict[x]) for x in inputs_dict.keys()}\n",
    "\n",
    "print('count quantiles: ',stats.quantiles(inputs_count.values()))\n",
    "print('score quantiles: ',stats.quantiles(inputs['confidenceScore']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd7d2c",
   "metadata": {},
   "source": [
    "# Threshold inputs based on quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_score_threshold = inputs.loc[(inputs['confidenceScore']>=4)]\n",
    "\n",
    "input_score_group = inputs.groupby('diseaseName')\n",
    "input_score_dict = {k:v for k,v in input_score_group}\n",
    "input_score_count = {x:len(input_score_dict[x]) for x in input_score_dict.keys()}\n",
    "\n",
    "inputs_count_threshold = {k:v  for (k,v) in input_score_count.items() if (v > 10)}\n",
    "\n",
    "inputs_combined_threshold = inputs_score_threshold.loc[inputs_score_threshold['diseaseName'].isin(list(inputs_count_threshold.keys()))]\n",
    "print(inputs_combined_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe289fa",
   "metadata": {},
   "source": [
    "# Map input IDs to STRING IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_api_url = \"https://version-12-0.string-db.org/api\"\n",
    "output_format = \"tsv-no-header\"\n",
    "method = \"get_string_ids\"\n",
    "str_params = {\n",
    "    \"identifiers\" : \"\\r\".join(list(inputs_combined_threshold['ENSP'])), \n",
    "    \"species\" : 9606, \n",
    "    \"echo_query\" : 1, \n",
    "}\n",
    "request_url = \"/\".join([string_api_url, output_format, method])\n",
    "string_results = requests.post(request_url, data=str_params)\n",
    "\n",
    "string_map = {}\n",
    "for line in string_results.text.strip().split(\"\\n\"):\n",
    "    l = line.split(\"\\t\")\n",
    "    string_map.update({l[0]:l[2]})\n",
    "string_df = pd.DataFrame.from_dict(string_map.items())\n",
    "string_df.columns = ['ENSP','str_id']\n",
    "\n",
    "inputs_string_df = inputs_combined_threshold.merge(string_df,on='ENSP',how ='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ad639",
   "metadata": {},
   "source": [
    "# Threshold TIGA data by gene count per disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e7f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiga_filtered = tiga_do[tiga_do['id'].isin(inputs_combined_threshold['diseaseID'])]\n",
    "\n",
    "tiga_group = tiga_filtered.groupby('trait')\n",
    "tiga_dict = {k:v for k,v in tiga_group}\n",
    "tiga_count = {x:len(tiga_dict[x]) for x in tiga_dict.keys()}\n",
    "tiga_count_threshold = {k:v  for (k,v) in tiga_count.items() if (v > 10)}\n",
    "\n",
    "tiga_threshold = tiga_filtered.loc[tiga_filtered['trait'].isin(list(tiga_count_threshold.keys()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a77f1f",
   "metadata": {},
   "source": [
    "# Map TIGA to STRING IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496a70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_api_url = \"https://version-12-0.string-db.org/api\"\n",
    "output_format = \"tsv-no-header\"\n",
    "method = \"get_string_ids\"\n",
    "str_params = {\n",
    "    \"identifiers\" : \"\\r\".join(list(tiga_threshold['ensemblId'])), \n",
    "    \"species\" : 9606, \n",
    "    \"echo_query\" : 1, \n",
    "}\n",
    "request_url = \"/\".join([string_api_url, output_format, method])\n",
    "string_results = requests.post(request_url, data=str_params)\n",
    "\n",
    "string_map = {}\n",
    "for line in string_results.text.strip().split(\"\\n\"):\n",
    "    l = line.split(\"\\t\")\n",
    "    string_map.update({l[0]:l[2]})\n",
    "string_df = pd.DataFrame.from_dict(string_map.items())\n",
    "string_df.columns = ['ENSP','str_id']\n",
    "\n",
    "tiga_string_df = tiga_threshold.merge(string_df,left_on = 'ensemblId',right_on = 'ENSP',how ='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166b91f",
   "metadata": {},
   "source": [
    "# Map inputs (txt and knowledge) to string IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b85c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_api_url = \"https://version-12-0.string-db.org/api\"\n",
    "output_format = \"tsv-no-header\"\n",
    "method = \"get_string_ids\"\n",
    "str_params = {\n",
    "    \"identifiers\" : \"\\r\".join(list(inputs_combined_threshold['ENSP'])), \n",
    "    \"species\" : 9606, \n",
    "    \"echo_query\" : 1, \n",
    "}\n",
    "request_url = \"/\".join([string_api_url, output_format, method])\n",
    "string_results = requests.post(request_url, data=str_params)\n",
    "\n",
    "string_map = {}\n",
    "for line in string_results.text.strip().split(\"\\n\"):\n",
    "    l = line.split(\"\\t\")\n",
    "    string_map.update({l[0]:l[2]})\n",
    "string_df = pd.DataFrame.from_dict(string_map.items())\n",
    "string_df.columns = ['ENSP','str_id']\n",
    "\n",
    "inputs_string_df = inputs_combined_threshold.merge(string_df,on='ENSP',how ='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b013c",
   "metadata": {},
   "source": [
    "# Generate input prize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfbf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiga_prizes = tiga_string_df.groupby('trait')\n",
    "tiga_prize_dict = {k:v for k,v in tiga_prizes}\n",
    "\n",
    "for disease in tiga_prize_dict.keys():\n",
    "    df = tiga_prize_dict[disease]\n",
    "    df = df[['str_id','n_snpw']]\n",
    "    df = df.rename(columns={'str_id':'NODEID','n_snpw':'prize'})\n",
    "    df.to_csv(f\"../prize_files/{disease.replace(' ','_')}_prizes.txt\",sep = '\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ca001",
   "metadata": {},
   "source": [
    "# Process STRING interactome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61860b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = pd.read_csv('../raw/9606.protein.links.v12.0.txt',sep = ' ',skiprows=[0],header=None)\n",
    "string = string[string.iloc[:,2]>900]\n",
    "string = string.iloc[:,[0,1]]\n",
    "string[len(string.columns)] = 1\n",
    "string.to_csv('string_interactome.txt',sep = ' ',index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41cbffa",
   "metadata": {},
   "source": [
    "# Generate gold standard files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8538e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_string_df = inputs_string_df[inputs_string_df['diseaseID'].isin(tiga_string_df['id'])]\n",
    "\n",
    "inputs_combined_group = inputs_string_df.groupby('diseaseName')\n",
    "inputs_combined_dict = {k:v for k,v in inputs_combined_group}\n",
    "\n",
    "for disease in inputs_combined_dict.keys():\n",
    "    df = inputs_combined_dict[disease]\n",
    "    df = df[['str_id']]\n",
    "    df.to_csv(f\"../GS_files/{disease.replace(' ','_')}_GS.txt\",sep = '\t',index=False,header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
